# -*- coding: utf-8 -*-
"""DataSetStats.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1idXjLrY-u3JlUGPdis2hA2yN-L2mBhiE
"""

import pandas as pd
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

train_csv_file = pd.read_csv("train_partition.csv")
valid_csv_file = pd.read_csv("val_partition.csv")
test_csv_file = pd.read_csv("test_partition.csv")

train, _ = train_test_split(train_csv_file, train_size=800, shuffle=False)
valid, _ = train_test_split(valid_csv_file, train_size=100, shuffle=False)
test, _ = train_test_split(test_csv_file, train_size=100, shuffle=False)


#training stats

training_sequence_lengths_error = {}
training_unique_words_error = {}
training_sequence_lengths_aln = {}
training_unique_words_aln = {}

for i in range(0,800):

  wordCount = 0
  tempErrorWords = str(train[train.columns[0]][i]).split()
  for potentialWord in tempErrorWords:
    if potentialWord.isalpha():
      wordCount += 1
      if potentialWord in training_unique_words_error.keys():
        training_unique_words_error[potentialWord] = training_unique_words_error[potentialWord] + 1
      else:
        training_unique_words_error[potentialWord] = 1
  if wordCount in training_sequence_lengths_error.keys():
    training_sequence_lengths_error[wordCount] = training_sequence_lengths_error[wordCount] + 1
  else:
    training_sequence_lengths_error[wordCount] = 1

  wordCount = 0
  tempAlnWords = str(train[train.columns[1]][i]).split()
  for potentialWord in tempAlnWords:
    if potentialWord.isalpha():
      wordCount += 1
      if potentialWord in training_unique_words_aln.keys():
        training_unique_words_aln[potentialWord] = training_unique_words_aln[potentialWord] + 1
      else:
        training_unique_words_aln[potentialWord] = 1
  if wordCount in training_sequence_lengths_aln.keys():
    training_sequence_lengths_aln[wordCount] = training_sequence_lengths_aln[wordCount] + 1
  else:
    training_sequence_lengths_aln[wordCount] = 1

#val stats
validation_sequence_lengths_error = {}
validation_unique_words_error = {}
validation_sequence_lengths_aln = {}
validation_unique_words_aln = {}

for i in range(0,100):

  wordCount = 0
  tempErrorWords = str(valid[valid.columns[0]][i]).split()
  for potentialWord in tempErrorWords:
    if potentialWord.isalpha():
      wordCount += 1
      if potentialWord in validation_unique_words_error.keys():
        validation_unique_words_error[potentialWord] = validation_unique_words_error[potentialWord] + 1
      else:
        validation_unique_words_error[potentialWord] = 1
  if wordCount in validation_sequence_lengths_error.keys():
    validation_sequence_lengths_error[wordCount] = validation_sequence_lengths_error[wordCount] + 1
  else:
    validation_sequence_lengths_error[wordCount] = 1

  wordCount = 0
  tempAlnWords = str(valid[valid.columns[1]][i]).split()
  for potentialWord in tempAlnWords:
    if potentialWord.isalpha():
      wordCount += 1
      if potentialWord in validation_unique_words_aln.keys():
        validation_unique_words_aln[potentialWord] = validation_unique_words_aln[potentialWord] + 1
      else:
        validation_unique_words_aln[potentialWord] = 1
  if wordCount in validation_sequence_lengths_aln.keys():
    validation_sequence_lengths_aln[wordCount] = validation_sequence_lengths_aln[wordCount] + 1
  else:
    validation_sequence_lengths_aln[wordCount] = 1

#test stats

testing_sequence_lengths_error = {}
testing_unique_words_error = {}
testing_sequence_lengths_aln = {}
testing_unique_words_aln = {}

for i in range(0,100):

  wordCount = 0
  tempErrorWords = str(test[test.columns[0]][i]).split()
  for potentialWord in tempErrorWords:
    if potentialWord.isalpha():
      wordCount += 1
      if potentialWord in testing_unique_words_error.keys():
        testing_unique_words_error[potentialWord] = testing_unique_words_error[potentialWord] + 1
      else:
        testing_unique_words_error[potentialWord] = 1
  if wordCount in testing_sequence_lengths_error.keys():
    testing_sequence_lengths_error[wordCount] = testing_sequence_lengths_error[wordCount] + 1
  else:
    testing_sequence_lengths_error[wordCount] = 1

  wordCount = 0
  tempAlnWords = str(test[test.columns[1]][i]).split()
  for potentialWord in tempAlnWords:
    if potentialWord.isalpha():
      wordCount += 1
      if potentialWord in testing_unique_words_aln.keys():
        testing_unique_words_aln[potentialWord] = testing_unique_words_aln[potentialWord] + 1
      else:
        testing_unique_words_aln[potentialWord] = 1
  if wordCount in testing_sequence_lengths_aln.keys():
    testing_sequence_lengths_aln[wordCount] = testing_sequence_lengths_aln[wordCount] + 1
  else:
    testing_sequence_lengths_aln[wordCount] = 1

plt.bar(training_sequence_lengths_error.keys(), training_sequence_lengths_error.values(), label="Sequence Lengths of Training Errors")
plt.show()
print()
plt.bar(validation_sequence_lengths_error.keys(), validation_sequence_lengths_error.values(), label="Sequence Lengths of Validation Errors")
plt.show()
print()
plt.bar(testing_sequence_lengths_error.keys(), testing_sequence_lengths_error.values(), label="Sequence Lengths of Testing Errors")
plt.show()
print()

training_unique_words_error = dict(sorted(training_unique_words_error.items()))
plt.ylim((0,700))
plt.bar(training_unique_words_error.keys(), training_unique_words_error.values(), tick_label=' ', label="Unique Word Frequency in Training Errors")
plt.show()
print()
validation_unique_words_error = dict(sorted(validation_unique_words_error.items()))
plt.bar(validation_unique_words_error.keys(), validation_unique_words_error.values(), tick_label=' ', label="Unique Word Frequency in Validation Errors")
plt.show()
print()
testing_unique_words_error = dict(sorted(testing_unique_words_error.items()))
plt.bar(testing_unique_words_error.keys(), testing_unique_words_error.values(), tick_label=' ', label="Unique Word Frequency in Testing Errors")
plt.show()
print()


plt.bar(training_sequence_lengths_aln.keys(), training_sequence_lengths_aln.values(), label="Sequence Lengths of Training Alignments")
plt.show()
print()
plt.bar(validation_sequence_lengths_aln.keys(), validation_sequence_lengths_aln.values(), label="Sequence Lengths of Validation Alignments")
plt.show()
print()
plt.bar(testing_sequence_lengths_aln.keys(), testing_sequence_lengths_aln.values(), label="Sequence Lengths of Testing Alignments")
plt.show()
print()


training_unique_words_aln = dict(sorted(training_unique_words_aln.items()))
plt.bar(training_unique_words_aln.keys(), training_unique_words_aln.values(), tick_label=' ', label="Unique Word Frequency in Training Alignments")
plt.show()
validation_unique_words_aln = dict(sorted(validation_unique_words_aln.items()))
plt.bar(validation_unique_words_aln.keys(), validation_unique_words_aln.values(), tick_label=' ', label="Unique Word Frequency in Validation Alignments")
plt.show()
testing_unique_words_aln = dict(sorted(testing_unique_words_aln.items()))
plt.bar(testing_unique_words_aln.keys(), testing_unique_words_aln.values(), tick_label=' ', label="Unique Word Frequency in Testing Alignments")
plt.show()